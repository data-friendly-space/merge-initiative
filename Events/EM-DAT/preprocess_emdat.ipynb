{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable SettingWithCopyWarning\n",
    "import warnings\n",
    "\n",
    "# Disable the SettingWithCopyWarning\n",
    "# Note: Disabling warnings should be done cautiously. Ensure all DataFrame modifications are intentional.\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Usage\n",
    "file_path = input(\"Enter the path to the Excel file: \")\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"DisNo.\",\n",
    "    \"Classification Key\",\n",
    "    \"Disaster Group\",\n",
    "    \"Disaster Subgroup\",\n",
    "    \"Disaster Type\",\n",
    "    \"Disaster Subtype\",\n",
    "    \"External IDs\",\n",
    "    \"Event Name\",\n",
    "    \"ISO\",\n",
    "    \"Country\",\n",
    "    \"Subregion\",\n",
    "    \"Region\",\n",
    "    \"Location\",\n",
    "    \"AID Contribution ('000 US$)\",\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "    \"River Basin\",\n",
    "    \"Start Year\",\n",
    "    \"Start Month\",\n",
    "    \"Start Day\",\n",
    "    \"End Year\",\n",
    "    \"End Month\",\n",
    "    \"End Day\",\n",
    "    \"Total Deaths\",\n",
    "    \"No. Injured\",\n",
    "    \"No. Affected\",\n",
    "    \"No. Homeless\",\n",
    "    \"Total Affected\",\n",
    "    \"Reconstruction Costs, Adjusted ('000 US$)\",\n",
    "    \"Total Damage, Adjusted ('000 US$)\",\n",
    "    \"Admin Units\",\n",
    "]\n",
    "\n",
    "# Filter the DataFrame to keep only the selected columns\n",
    "df_filtered = df[selected_columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Conbine dates from `Start Year/Month/Day` and `End Year/Month/Day` into a single datetime column\n",
    "2. Ensure the end date is not earlier than the start date\n",
    "3. Add a new column `start_date` and `end_date` to the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Combine start date components into a single datetime column, use 1 for missing values\n",
    "df_filtered[\"start_date\"] = pd.to_datetime(\n",
    "    df_filtered[\"Start Year\"].astype(int).astype(str)\n",
    "    + \"-\"\n",
    "    + df_filtered[\"Start Month\"].fillna(1).astype(int).astype(str).str.zfill(2)\n",
    "    + \"-\"\n",
    "    + df_filtered[\"Start Day\"].fillna(1).astype(int).astype(str).str.zfill(2),\n",
    "    format=\"%Y-%m-%d\",\n",
    "    errors=\"coerce\",  # Handle invalid dates gracefully\n",
    ")\n",
    "\n",
    "# Combine end date components into a single datetime column, use 1 for missing values\n",
    "df_filtered[\"end_date\"] = pd.to_datetime(\n",
    "    df_filtered[\"End Year\"].astype(int).astype(str)\n",
    "    + \"-\"\n",
    "    + df_filtered[\"End Month\"].fillna(1).astype(int).astype(str).str.zfill(2)\n",
    "    + \"-\"\n",
    "    + df_filtered[\"End Day\"].fillna(1).astype(int).astype(str).str.zfill(2),\n",
    "    format=\"%Y-%m-%d\",\n",
    "    errors=\"coerce\",  # Handle invalid dates gracefully\n",
    ")\n",
    "\n",
    "# Check if end_date > start_date, if not, change end_date to start_date\n",
    "df_filtered[\"end_date\"] = np.where(\n",
    "    df_filtered[\"end_date\"] < df_filtered[\"start_date\"],\n",
    "    df_filtered[\"start_date\"],\n",
    "    df_filtered[\"end_date\"],\n",
    ")\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "print(\"\\nFirst few rows after date correction:\")\n",
    "display(df_filtered[[\"start_date\", \"end_date\"]].head())\n",
    "\n",
    "# Count the number of rows where end_date was changed\n",
    "changed_rows = (df_filtered[\"end_date\"] == df_filtered[\"start_date\"]).sum()\n",
    "print(\n",
    "    f\"\\nNumber of rows where end_date was changed to match start_date: {changed_rows}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered[\n",
    "    [\n",
    "        \"Start Year\",\n",
    "        \"Start Month\",\n",
    "        \"Start Day\",\n",
    "        \"End Year\",\n",
    "        \"End Month\",\n",
    "        \"End Day\",\n",
    "        \"start_date\",\n",
    "        \"end_date\",\n",
    "    ]\n",
    "].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extract GLIDE number from `External IDs`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract External IDs and create new columns\n",
    "def extract_external_ids(external_id):\n",
    "    \"\"\"\n",
    "    Extract the External ID types and values from the given string.\n",
    "\n",
    "    Args:\n",
    "    external_id (str): The External ID string to process.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the External ID types as keys and lists of values as values.\n",
    "    \"\"\"\n",
    "    if pd.isna(external_id):\n",
    "        return {}\n",
    "    id_dict = {}\n",
    "    for id_pair in external_id.split(\"|\"):\n",
    "        parts = id_pair.split(\":\", 1)\n",
    "        if len(parts) == 2:\n",
    "            id_type, id_value = parts\n",
    "            if id_type in id_dict:\n",
    "                id_dict[id_type].append(id_value)\n",
    "            else:\n",
    "                id_dict[id_type] = [id_value]\n",
    "    return id_dict\n",
    "\n",
    "\n",
    "# Apply the function to create new columns\n",
    "df_filtered[\"ExternalIDs\"] = df_filtered[\"External IDs\"].apply(\n",
    "    extract_external_ids\n",
    ")\n",
    "\n",
    "# Create separate columns for each External ID type\n",
    "external_id_types = set()\n",
    "for id_dict in df_filtered[\"ExternalIDs\"]:\n",
    "    external_id_types.update(id_dict.keys())\n",
    "\n",
    "for id_type in external_id_types:\n",
    "    df_filtered[id_type] = df_filtered[\"ExternalIDs\"].apply(\n",
    "        lambda x: x.get(id_type, None)\n",
    "    )\n",
    "\n",
    "# Drop the temporary column\n",
    "df_filtered = df_filtered.drop(columns=[\"ExternalIDs\"])\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "print(\"\\nFirst few rows after External ID extraction:\")\n",
    "display(df_filtered[[\"External IDs\"] + list(external_id_types)].head())\n",
    "\n",
    "# Count the number of non-null values in each new column\n",
    "print(\"\\nNumber of non-null values in each new External ID column:\")\n",
    "display(df_filtered[list(external_id_types)].count())\n",
    "\n",
    "display(df_filtered[df_filtered[\"GLIDE\"].notna()].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extract Admin 1 and 2 level and names from `Admin Units`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered[df_filtered[\"Admin Units\"].notna()].head()\n",
    "\n",
    "import ast\n",
    "\n",
    "\n",
    "def extract_admin_levels(admin_units):\n",
    "    \"\"\"\n",
    "    Extract admin level 1 and 2 names from the Admin Units string.\n",
    "\n",
    "    Args:\n",
    "    admin_units (str): The Admin Units string to process.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing lists of admin level 1 and 2 names.\n",
    "    \"\"\"\n",
    "    if pd.isna(admin_units):\n",
    "        return [], []\n",
    "\n",
    "    # Convert string representation of list to actual list of dictionaries\n",
    "    try:\n",
    "        admin_list = ast.literal_eval(admin_units)\n",
    "    except:\n",
    "        return [], []\n",
    "\n",
    "    admin_level_1 = []\n",
    "    admin_level_2 = []\n",
    "\n",
    "    for admin in admin_list:\n",
    "        if \"adm1_name\" in admin:\n",
    "            admin_level_1.append(admin[\"adm1_name\"])\n",
    "        elif \"adm2_name\" in admin:\n",
    "            admin_level_2.append(admin[\"adm2_name\"])\n",
    "\n",
    "    return admin_level_1, admin_level_2\n",
    "\n",
    "\n",
    "# Apply the function to create new columns\n",
    "df_filtered[[\"admin_level_1\", \"admin_level_2\"]] = df_filtered[\n",
    "    \"Admin Units\"\n",
    "].apply(lambda x: pd.Series(extract_admin_levels(x)))\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "print(\"\\nFirst few rows after Admin Units extraction:\")\n",
    "display(df_filtered[[\"Admin Units\", \"admin_level_1\", \"admin_level_2\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transform unit from '000 US$ to US$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns_to_transform = [\n",
    "    \"AID Contribution ('000 US$)\",\n",
    "    \"Reconstruction Costs, Adjusted ('000 US$)\",\n",
    "    \"Total Damage, Adjusted ('000 US$)\",\n",
    "]\n",
    "\n",
    "# Transform unit from '000 US$ to US$\n",
    "for column in selected_columns_to_transform:\n",
    "    # Check if the column exists in the DataFrame\n",
    "    if column in df_filtered.columns:\n",
    "        # Convert '000 US$ to US$ by multiplying by 1000\n",
    "        df_filtered[column] = df_filtered[column].multiply(1000)\n",
    "\n",
    "        # Rename the column to reflect the new unit and format\n",
    "        new_column_name = (\n",
    "            column.replace(\"('000 US$)\", \"\")\n",
    "            .strip()\n",
    "            .lower()\n",
    "            .replace(\", \", \"_\")\n",
    "            .replace(\" \", \"_\")\n",
    "        )\n",
    "        df_filtered.rename(columns={column: new_column_name}, inplace=True)\n",
    "\n",
    "        # Add inline comment to explain the transformation\n",
    "        print(\n",
    "            f\"Transformed {column} to {new_column_name}\"\n",
    "        )  # Conversion from '000 US$ to US$\n",
    "    else:\n",
    "        print(f\"Column {column} not found in the DataFrame\")\n",
    "\n",
    "# Display the first few rows of the transformed columns to verify changes\n",
    "print(\"\\nFirst few rows after unit transformation:\")\n",
    "display(\n",
    "    df_filtered[\n",
    "        [\n",
    "            col.replace(\"('000 US$)\", \"\")\n",
    "            .strip()\n",
    "            .lower()\n",
    "            .replace(\", \", \"_\")\n",
    "            .replace(\" \", \"_\")\n",
    "            for col in selected_columns_to_transform\n",
    "            if col.replace(\"('000 US$)\", \"\")\n",
    "            .strip()\n",
    "            .lower()\n",
    "            .replace(\", \", \"_\")\n",
    "            .replace(\" \", \"_\")\n",
    "            in df_filtered.columns\n",
    "        ]\n",
    "    ].head()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transform ISO3, Admin name, and Admin level to GADM's standard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the first 10 rows from the Excel file\n",
    "emdat_admin_mapping = pd.read_excel(\n",
    "    \"./EMDAT_admin_area_mapping.xlsx\", nrows=10\n",
    ")\n",
    "\n",
    "# Display the extracted rows\n",
    "print(\"First 10 rows of EMDAT admin area mapping:\")\n",
    "display(emdat_admin_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the full EMDAT admin area mapping\n",
    "emdat_admin_mapping = pd.read_excel(\"./EMDAT_admin_area_mapping.xlsx\")\n",
    "\n",
    "\n",
    "# Update country names and ISO codes\n",
    "def update_country_info(row):\n",
    "    \"\"\"\n",
    "    Update the country name and ISO code based on GADM's definition.\n",
    "\n",
    "    Args:\n",
    "    row (pandas.Series): A row from the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Updated ISO code, country name, and admin levels.\n",
    "    \"\"\"\n",
    "    iso = row[\"ISO\"]\n",
    "    country = row[\"Country\"]\n",
    "    admin1 = (\n",
    "        row[\"admin_level_1\"] if isinstance(row[\"admin_level_1\"], list) else []\n",
    "    )\n",
    "    admin2 = (\n",
    "        row[\"admin_level_2\"] if isinstance(row[\"admin_level_2\"], list) else []\n",
    "    )\n",
    "\n",
    "    # Special cases\n",
    "    if iso in [\"HKG\", \"MAC\"]:\n",
    "        admin1.append(country)\n",
    "        return \"CHN\", \"China\", admin1, admin2\n",
    "    elif iso == \"ANT\":\n",
    "        admin1.append(country)\n",
    "        return \"NLD\", \"Netherlands\", admin1, admin2\n",
    "    elif iso == \"SCG\":\n",
    "        # Check if any admin1 or admin2 names match Serbia's or Montenegro's locations\n",
    "        srb_locations = set()\n",
    "        mne_locations = set()\n",
    "\n",
    "        for country_code in [\"SRB\", \"MNE\"]:\n",
    "            country_data = emdat_admin_mapping[\n",
    "                emdat_admin_mapping[\"ISO3\"] == country_code\n",
    "            ]\n",
    "            locations = set()\n",
    "\n",
    "            # Handle GADM_Admin1 and GADM_Admin2\n",
    "            for col in [\"GADM_Admin1\", \"GADM_Admin2\"]:\n",
    "                locations.update(country_data[col].dropna().tolist())\n",
    "\n",
    "            # Handle alternative and local names\n",
    "            for col in [\n",
    "                \"GADM_Admin1_Alt\",\n",
    "                \"GADM_Admin2_Alt\",\n",
    "                \"GADM_Admin1_Local\",\n",
    "                \"GADM_Admin2_Local\",\n",
    "            ]:\n",
    "                locations.update(\n",
    "                    [\n",
    "                        item\n",
    "                        for sublist in country_data[col]\n",
    "                        .dropna()\n",
    "                        .str.split(\"|\")\n",
    "                        for item in sublist\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            if country_code == \"SRB\":\n",
    "                srb_locations = locations\n",
    "            else:\n",
    "                mne_locations = locations\n",
    "\n",
    "        all_locations = set(admin1 + admin2)\n",
    "\n",
    "        if any(loc in srb_locations for loc in all_locations):\n",
    "            return \"SRB\", \"Serbia\", admin1, admin2\n",
    "        elif any(loc in mne_locations for loc in all_locations):\n",
    "            return \"MNE\", \"Montenegro\", admin1, admin2\n",
    "        else:\n",
    "            # If no match found, return None to indicate this row should be dropped\n",
    "            return None\n",
    "    elif iso == \"AB9\":\n",
    "        admin2.append(\"Abyei Area\")\n",
    "        return \"SDN\", \"Sudan\", admin1, admin2\n",
    "    elif iso == \"XKK\":\n",
    "        return \"XKO\", \"Kosovo\", admin1, admin2\n",
    "\n",
    "    # General case\n",
    "    match = emdat_admin_mapping[emdat_admin_mapping[\"ISO3\"] == iso]\n",
    "    if not match.empty:\n",
    "        return iso, match.iloc[0][\"GADM_Country\"], admin1, admin2\n",
    "    return iso, country, admin1, admin2\n",
    "\n",
    "\n",
    "# Update country names and ISO codes\n",
    "df_filtered[[\"ISO\", \"Country\", \"admin_level_1\", \"admin_level_2\"]] = (\n",
    "    df_filtered.apply(update_country_info, axis=1, result_type=\"expand\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where update_country_info returned None (for unmatched SCG cases)\n",
    "df_filtered = df_filtered.dropna(subset=[\"ISO\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a name matches any of the alternatives\n",
    "def match_name(name, alternatives):\n",
    "    \"\"\"\n",
    "    Check if a name matches any of the alternatives.\n",
    "\n",
    "    Args:\n",
    "    name (str): The name to check.\n",
    "    alternatives (str): Pipe-separated string of alternative names.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if there's a match, False otherwise.\n",
    "    \"\"\"\n",
    "    if pd.isna(alternatives):\n",
    "        return False\n",
    "    alt_list = alternatives.split(\"|\")\n",
    "    return name in alt_list\n",
    "\n",
    "\n",
    "# Update admin level 1 names\n",
    "def update_admin1_name(row, admin1_name):\n",
    "    \"\"\"\n",
    "    Update the admin level 1 name based on ISO3 code and alternative names.\n",
    "\n",
    "    Args:\n",
    "    row (pandas.Series): A row from the DataFrame.\n",
    "    admin1_name (str): The original admin1 name.\n",
    "\n",
    "    Returns:\n",
    "    str: Updated admin1 name or original name if no match found.\n",
    "    \"\"\"\n",
    "    matches = emdat_admin_mapping[\n",
    "        (emdat_admin_mapping[\"ISO3\"] == row[\"ISO\"])\n",
    "        & (\n",
    "            (emdat_admin_mapping[\"GADM_Admin1\"] == admin1_name)\n",
    "            | (\n",
    "                emdat_admin_mapping[\"GADM_Admin1_Alt\"].apply(\n",
    "                    lambda x: match_name(admin1_name, x)\n",
    "                )\n",
    "            )\n",
    "            | (\n",
    "                emdat_admin_mapping[\"GADM_Admin1_Local\"].apply(\n",
    "                    lambda x: match_name(admin1_name, x)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "    if not matches.empty:\n",
    "        return matches.iloc[0][\"GADM_Admin1\"]\n",
    "    return admin1_name\n",
    "\n",
    "\n",
    "# Update admin level 1 names\n",
    "df_filtered[\"admin_level_1\"] = df_filtered.apply(\n",
    "    lambda row: [\n",
    "        update_admin1_name(row, name) for name in row[\"admin_level_1\"]\n",
    "    ]\n",
    "    if isinstance(row[\"admin_level_1\"], list)\n",
    "    else row[\"admin_level_1\"],\n",
    "    axis=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update admin level 2 names\n",
    "def update_admin2_name(row, admin2_name):\n",
    "    \"\"\"\n",
    "    Update the admin level 2 name based on ISO3 code and alternative names.\n",
    "\n",
    "    Args:\n",
    "    row (pandas.Series): A row from the DataFrame.\n",
    "    admin2_name (str): The original admin2 name.\n",
    "\n",
    "    Returns:\n",
    "    str: Updated admin2 name or original name if no match found.\n",
    "    \"\"\"\n",
    "    matches = emdat_admin_mapping[\n",
    "        (emdat_admin_mapping[\"ISO3\"] == row[\"ISO\"])\n",
    "        & (\n",
    "            (emdat_admin_mapping[\"GADM_Admin2\"] == admin2_name)\n",
    "            | (\n",
    "                emdat_admin_mapping[\"GADM_Admin2_Alt\"].apply(\n",
    "                    lambda x: match_name(admin2_name, x)\n",
    "                )\n",
    "            )\n",
    "            | (\n",
    "                emdat_admin_mapping[\"GADM_Admin2_Local\"].apply(\n",
    "                    lambda x: match_name(admin2_name, x)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "    if not matches.empty:\n",
    "        return matches.iloc[0][\"GADM_Admin2\"]\n",
    "    return admin2_name\n",
    "\n",
    "\n",
    "# Update admin level 2 names\n",
    "df_filtered[\"admin_level_2\"] = df_filtered.apply(\n",
    "    lambda row: [\n",
    "        update_admin2_name(row, name) for name in row[\"admin_level_2\"]\n",
    "    ]\n",
    "    if isinstance(row[\"admin_level_2\"], list)\n",
    "    else row[\"admin_level_2\"],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Display the first few rows to verify changes\n",
    "print(\"First few rows after updating country, ISO, admin1, and admin2 names:\")\n",
    "display(\n",
    "    df_filtered[[\"ISO\", \"Country\", \"admin_level_1\", \"admin_level_2\"]].head()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns based on the provided mapping\n",
    "column_mapping = {\n",
    "    \"Disaster Group\": \"disaster_group\",\n",
    "    \"Disaster Subgroup\": \"disaster_subgroup\",\n",
    "    \"Disaster Type\": \"disaster_type\",\n",
    "    \"Disaster Subtype\": \"disaster_subtype\",\n",
    "    \"ISO\": \"iso3_code\",\n",
    "    \"Country\": \"admin_level_0\",\n",
    "    \"Total Deaths\": \"total_deaths\",\n",
    "    \"No. Injured\": \"number_injured\",\n",
    "    \"No. Affected\": \"number_affected\",\n",
    "    \"No. Homeless\": \"number_homeless\",\n",
    "    \"Total Affected\": \"total_affected\",\n",
    "}\n",
    "\n",
    "# Rename the columns in df_filtered\n",
    "df_filtered.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# Display the first few rows to verify changes\n",
    "print(\"First few rows after renaming columns:\")\n",
    "display(df_filtered[list(column_mapping.values())].head())\n",
    "\n",
    "# Display column names to confirm changes\n",
    "print(\"\\nUpdated column names:\")\n",
    "print(df_filtered.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the columns to be included in the metadata\n",
    "metadata_columns = [\n",
    "    \"DisNo.\",\n",
    "    \"Classification Key\",\n",
    "    \"Event Name\",\n",
    "    \"External IDs\",\n",
    "    \"Subregion\",\n",
    "    \"Region\",\n",
    "    \"Location\",\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "    \"River Basin\",\n",
    "    \"Start Year\",\n",
    "    \"Start Month\",\n",
    "    \"Start Day\",\n",
    "    \"End Year\",\n",
    "    \"End Month\",\n",
    "    \"End Day\",\n",
    "    \"Admin Units\",\n",
    "]\n",
    "\n",
    "\n",
    "# Function to create metadata JSON\n",
    "def create_metadata(row):\n",
    "    \"\"\"\n",
    "    Create a JSON string containing metadata from specified columns.\n",
    "\n",
    "    Args:\n",
    "    row (pandas.Series): A row from the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    str: JSON string containing metadata.\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        col: row[col] for col in metadata_columns if pd.notna(row[col])\n",
    "    }\n",
    "    return json.dumps(metadata)\n",
    "\n",
    "\n",
    "# Create the metadata column\n",
    "df_filtered[\"metadata\"] = df_filtered.apply(create_metadata, axis=1)\n",
    "\n",
    "# Display the first few rows to verify the new metadata column\n",
    "print(\"First few rows after adding metadata column:\")\n",
    "display(df_filtered[[\"metadata\"] + list(column_mapping.values())].head())\n",
    "\n",
    "# Remove the original columns that are now in metadata\n",
    "df_filtered = df_filtered.drop(columns=metadata_columns)\n",
    "\n",
    "print(\"\\nUpdated column names after removing metadata columns:\")\n",
    "print(df_filtered.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'source' column with value 'EMDAT'\n",
    "df_filtered[\"source\"] = \"EMDAT\"\n",
    "\n",
    "# Display the first few rows to verify the new column\n",
    "print(\"First few rows after adding 'source' column:\")\n",
    "display(df_filtered[[\"source\"] + df_filtered.columns[:-1].tolist()].head())\n",
    "\n",
    "# Confirm the new column is added\n",
    "print(\"\\nUpdated column names:\")\n",
    "print(df_filtered.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "\n",
    "def generate_short_uuid():\n",
    "    \"\"\"Generate a short UUID.\"\"\"\n",
    "    return str(uuid.uuid4())[:8]\n",
    "\n",
    "\n",
    "def create_event_name(row):\n",
    "    \"\"\"\n",
    "    Create an event name based on the specified pattern.\n",
    "\n",
    "    Args:\n",
    "    row (pandas.Series): A row from the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    str: Event name string.\n",
    "    \"\"\"\n",
    "    # Define default values for missing data\n",
    "    iso3 = (\n",
    "        row[\"iso3_code\"] if pd.notna(row[\"iso3_code\"]) else \"UNKNOWN-ISO3-CODE\"\n",
    "    )\n",
    "    disaster_type = (\n",
    "        row[\"disaster_type\"]\n",
    "        if pd.notna(row[\"disaster_type\"])\n",
    "        else \"UNKNOWN-DISASTER-TYPE\"\n",
    "    )\n",
    "    disaster_subtype = (\n",
    "        row[\"disaster_subtype\"]\n",
    "        if pd.notna(row[\"disaster_subtype\"])\n",
    "        else \"UNKNOWN-DISASTER-SUBTYPE\"\n",
    "    )\n",
    "\n",
    "    # Convert start_date to string format YYYYMMDD\n",
    "    start_time = (\n",
    "        row[\"start_date\"].strftime(\"%Y%m%d\")\n",
    "        if pd.notna(row[\"start_date\"])\n",
    "        else \"UNKNOWN-START-DATE\"\n",
    "    )\n",
    "\n",
    "    # Generate short UUID\n",
    "    short_uuid = generate_short_uuid()\n",
    "\n",
    "    # Create event name\n",
    "    event_name = (\n",
    "        f\"{iso3}_{disaster_type}_{disaster_subtype}_{start_time}_{short_uuid}\"\n",
    "    )\n",
    "\n",
    "    return event_name\n",
    "\n",
    "\n",
    "# Create the event_name column\n",
    "df_filtered[\"event_name\"] = df_filtered.apply(create_event_name, axis=1)\n",
    "\n",
    "# Display the first few rows to verify the new event_name column\n",
    "print(\"First few rows after adding event_name column:\")\n",
    "display(df_filtered[[\"event_name\"] + df_filtered.columns[:-1].tolist()].head())\n",
    "\n",
    "# Confirm the new column is added\n",
    "print(\"\\nUpdated column names:\")\n",
    "print(df_filtered.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final df_filtered to a CSV file\n",
    "csv_filename = \"preprocessed_emdat.csv\"\n",
    "df_filtered.to_csv(csv_filename, index=False)\n",
    "print(f\"DataFrame saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "\n",
    "def insert_emdat_data(df):\n",
    "    \"\"\"\n",
    "    Insert the preprocessed EM-DAT data into the events_emdat table.\n",
    "\n",
    "    Args:\n",
    "    df (pandas.DataFrame): The preprocessed EM-DAT data.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Connect to the local PostgreSQL database\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=\"merge\",\n",
    "            user=\"postgres\",\n",
    "            password=getpass(\"Enter the database password: \"),\n",
    "            host=input(\"Enter the database host: \"),\n",
    "        )\n",
    "\n",
    "        # Create a cursor object\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Prepare the data for insertion\n",
    "        data = df.to_dict(\"records\")\n",
    "\n",
    "        # SQL query for inserting data\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO events_emdat (\n",
    "            event_name, disaster_group, disaster_subgroup, disaster_type, disaster_subtype,\n",
    "            iso3_code, admin_level_0, admin_level_1, admin_level_2, start_date, end_date,\n",
    "            total_deaths, number_injured, number_affected, number_homeless, total_affected,\n",
    "            total_damage_adjusted, reconstruction_costs_adjusted, aid_contribution,\n",
    "            source, metadata, USGS, GLIDE, DFO\n",
    "        ) VALUES %s\n",
    "        ON CONFLICT (event_name, iso3_code, start_date) DO NOTHING\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert data to tuple format for psycopg2, handling potential integer overflow\n",
    "        values = [\n",
    "            (\n",
    "                row[\"event_name\"],\n",
    "                row[\"disaster_group\"],\n",
    "                row[\"disaster_subgroup\"],\n",
    "                row[\"disaster_type\"],\n",
    "                row[\"disaster_subtype\"],\n",
    "                row[\"iso3_code\"],\n",
    "                row[\"admin_level_0\"],\n",
    "                row[\"admin_level_1\"],\n",
    "                row[\"admin_level_2\"],\n",
    "                row[\"start_date\"],\n",
    "                row[\"end_date\"],\n",
    "                int(row[\"total_deaths\"])\n",
    "                if pd.notna(row[\"total_deaths\"])\n",
    "                else None,\n",
    "                int(row[\"number_injured\"])\n",
    "                if pd.notna(row[\"number_injured\"])\n",
    "                else None,\n",
    "                int(row[\"number_affected\"])\n",
    "                if pd.notna(row[\"number_affected\"])\n",
    "                else None,\n",
    "                int(row[\"number_homeless\"])\n",
    "                if pd.notna(row[\"number_homeless\"])\n",
    "                else None,\n",
    "                int(row[\"total_affected\"])\n",
    "                if pd.notna(row[\"total_affected\"])\n",
    "                else None,\n",
    "                float(row[\"total_damage_adjusted\"])\n",
    "                if pd.notna(row[\"total_damage_adjusted\"])\n",
    "                else None,\n",
    "                float(row[\"reconstruction_costs_adjusted\"])\n",
    "                if pd.notna(row[\"reconstruction_costs_adjusted\"])\n",
    "                else None,\n",
    "                float(row[\"aid_contribution\"])\n",
    "                if pd.notna(row[\"aid_contribution\"])\n",
    "                else None,\n",
    "                row[\"source\"],\n",
    "                row[\"metadata\"],\n",
    "                row[\"USGS\"],\n",
    "                row[\"GLIDE\"],\n",
    "                row[\"DFO\"],\n",
    "            )\n",
    "            for row in data\n",
    "        ]\n",
    "\n",
    "        # Execute the insert query\n",
    "        execute_values(cur, insert_query, values)\n",
    "\n",
    "        # Commit the changes\n",
    "        conn.commit()\n",
    "\n",
    "        print(\"Data inserted successfully into events_emdat table.\")\n",
    "\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(f\"Error inserting data into events_emdat table: {error}\")\n",
    "\n",
    "    finally:\n",
    "        # Close the cursor and connection\n",
    "        if cur:\n",
    "            cur.close()\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "\n",
    "# Call the function to insert the data\n",
    "insert_emdat_data(df_filtered)\n",
    "\n",
    "# Print a message to confirm the operation is complete\n",
    "print(\"EM-DAT data insertion process completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# show info of the final df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about the filtered DataFrame\n",
    "print(\"Filtered DataFrame Info:\")\n",
    "df_filtered.info()\n",
    "\n",
    "# Display the first few rows of the filtered DataFrame\n",
    "print(\"\\nFirst few rows of the filtered DataFrame:\")\n",
    "display(df_filtered.head())\n",
    "\n",
    "# Display summary statistics for the filtered DataFrame\n",
    "print(\"\\nSummary statistics for the filtered DataFrame:\")\n",
    "display(df_filtered.describe())\n",
    "\n",
    "# Count non-null values for each column in the filtered DataFrame\n",
    "print(\"\\nNon-null value counts for each column:\")\n",
    "display(df_filtered.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
