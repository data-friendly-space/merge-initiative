{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable SettingWithCopyWarning\n",
    "import warnings\n",
    "\n",
    "# Disable the SettingWithCopyWarning\n",
    "# Note: Disabling warnings should be done cautiously. Ensure all DataFrame modifications are intentional.\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Usage\n",
    "file_path = input(\"Enter the path to the Excel file: \")\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"ISO3\",\n",
    "    \"Country / Territory\",\n",
    "    \"Year\",\n",
    "    \"Event Name\",\n",
    "    \"Date of Event (start)\",\n",
    "    \"Disaster Internal Displacements\",\n",
    "    \"Hazard Type\",\n",
    "    \"Hazard Sub Type\",\n",
    "    \"Event Codes (Code:Type)\",\n",
    "]\n",
    "\n",
    "# Filter the DataFrame to keep only the selected columns\n",
    "df_filtered = df[selected_columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extract GLIDE number from `Events Codes`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered[df_filtered[\"Event Codes (Code:Type)\"].notnull()].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Event Codes and create new columns\n",
    "def extract_event_codes(event_codes):\n",
    "    \"\"\"\n",
    "    Extract the Event Code types and values from the given string.\n",
    "\n",
    "    Args:\n",
    "    event_codes (str): The Event Codes string to process.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the Event Code types as keys and lists of values as values.\n",
    "    \"\"\"\n",
    "    if pd.isna(event_codes):\n",
    "        return {}\n",
    "    code_dict = {}\n",
    "    for code_pair in event_codes.split(\"; \"):\n",
    "        # Special handling for IFRC Appeal ID\n",
    "        if \":IFRC Appeal ID\" in code_pair:\n",
    "            code_value = code_pair.split(\":IFRC Appeal ID\")[0]\n",
    "            code_type = \"IFRC Appeal ID\"\n",
    "        else:\n",
    "            parts = code_pair.split(\":\", 1)\n",
    "            if len(parts) == 2:\n",
    "                code_value, code_type = parts\n",
    "            else:\n",
    "                continue  # Skip if the format is unexpected\n",
    "\n",
    "        if code_type in code_dict:\n",
    "            code_dict[code_type].append(code_value)\n",
    "        else:\n",
    "            code_dict[code_type] = [code_value]\n",
    "    return code_dict\n",
    "\n",
    "\n",
    "# Apply the function to create new columns\n",
    "df_filtered[\"EventCodes\"] = df_filtered[\"Event Codes (Code:Type)\"].apply(\n",
    "    extract_event_codes\n",
    ")\n",
    "\n",
    "# Create separate columns for each Event Code type\n",
    "event_code_types = set()\n",
    "for code_dict in df_filtered[\"EventCodes\"]:\n",
    "    event_code_types.update(code_dict.keys())\n",
    "\n",
    "for code_type in event_code_types:\n",
    "    df_filtered[code_type] = df_filtered[\"EventCodes\"].apply(\n",
    "        lambda x: x.get(code_type, None)\n",
    "    )\n",
    "\n",
    "# Drop the temporary column\n",
    "df_filtered = df_filtered.drop(columns=[\"EventCodes\"])\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "print(\"\\nFirst few rows after Event Code extraction:\")\n",
    "display(\n",
    "    df_filtered[[\"Event Codes (Code:Type)\"] + list(event_code_types)].head()\n",
    ")\n",
    "\n",
    "# Count the number of non-null values in each new column\n",
    "print(\"\\nNumber of non-null values in each new Event Code column:\")\n",
    "display(df_filtered[list(event_code_types)].count())\n",
    "\n",
    "# Display rows where Glide Number is not null\n",
    "print(\"\\nRows where Glide Number is not null:\")\n",
    "display(df_filtered[df_filtered[\"Glide Number\"].notna()].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process Glide Number\n",
    "def process_glide_number(glide_number):\n",
    "    \"\"\"\n",
    "    Process Glide Number by removing the last dash and checking the ISO3 code.\n",
    "\n",
    "    Args:\n",
    "    glide_number (str or list): The Glide Number string or list of strings.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Processed Glide Number(s) and a flag indicating if the ISO3 code(s) are valid.\n",
    "    \"\"\"\n",
    "    if isinstance(glide_number, list):\n",
    "        # Process each Glide Number in the list\n",
    "        processed_glides = []\n",
    "        valid_iso3s = []\n",
    "        for gn in glide_number:\n",
    "            processed, valid = process_single_glide(gn)\n",
    "            processed_glides.append(processed)\n",
    "            valid_iso3s.append(valid)\n",
    "        return processed_glides, all(valid_iso3s)\n",
    "    else:\n",
    "        # Process a single Glide Number\n",
    "        return process_single_glide(glide_number)\n",
    "\n",
    "\n",
    "def process_single_glide(glide_number):\n",
    "    \"\"\"Helper function to process a single Glide Number\"\"\"\n",
    "    if pd.isna(glide_number) or not isinstance(glide_number, str):\n",
    "        return None, True\n",
    "\n",
    "    parts = glide_number.split(\"-\")\n",
    "    if len(parts) < 2:\n",
    "        return glide_number, True\n",
    "\n",
    "    processed_glide = \"-\".join(\n",
    "        parts[:-1]\n",
    "    )  # Join all parts except the last one\n",
    "    iso3_code = parts[-1]  # Last part is the potential ISO3 code\n",
    "\n",
    "    # Check if the last part is a valid ISO3 code (3 uppercase letters)\n",
    "    is_valid_iso3 = (\n",
    "        len(iso3_code) == 3 and iso3_code.isupper() and iso3_code.isalpha()\n",
    "    )\n",
    "\n",
    "    return processed_glide, is_valid_iso3\n",
    "\n",
    "\n",
    "# Apply the function to the Glide Number column\n",
    "df_filtered[\"Processed_Glide_Number\"], df_filtered[\"Valid_ISO3\"] = zip(\n",
    "    *df_filtered[\"Glide Number\"].apply(process_glide_number)\n",
    ")\n",
    "\n",
    "# Print Glide Numbers with invalid ISO3 codes\n",
    "invalid_glide_numbers = df_filtered[~df_filtered[\"Valid_ISO3\"]]\n",
    "if not invalid_glide_numbers.empty:\n",
    "    print(\"\\nGlide Numbers with potentially invalid ISO3 codes:\")\n",
    "    for _, row in invalid_glide_numbers.iterrows():\n",
    "        print(\n",
    "            f\"Original Glide Number: {row['Glide Number']}, Processed: {row['Processed_Glide_Number']}\"\n",
    "        )\n",
    "else:\n",
    "    print(\"\\nAll Glide Numbers have valid ISO3 codes.\")\n",
    "\n",
    "# Update the Glide Number column with processed values\n",
    "df_filtered[\"Glide Number\"] = df_filtered[\"Processed_Glide_Number\"]\n",
    "\n",
    "# Drop temporary columns\n",
    "df_filtered = df_filtered.drop(\n",
    "    columns=[\"Processed_Glide_Number\", \"Valid_ISO3\"]\n",
    ")\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "print(\"\\nFirst few rows after Glide Number processing:\")\n",
    "display(df_filtered[[\"Glide Number\"]].head())\n",
    "\n",
    "# Count the number of non-null Glide Numbers\n",
    "print(\n",
    "    f\"\\nNumber of non-null Glide Numbers: {df_filtered['Glide Number'].count()}\"\n",
    ")\n",
    "\n",
    "# Display rows where Glide Number is not null\n",
    "print(\"\\nRows where Glide Number is not null:\")\n",
    "display(df_filtered[df_filtered[\"Glide Number\"].notna()].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extract location names from `Event Name`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the ISO of IDMC to match GADM\n",
    "# Function to modify ISO3 and Country/Territory to match GADM\n",
    "def modify_iso_and_country(row):\n",
    "    \"\"\"\n",
    "    Modify ISO3 and Country/Territory to match GADM standards.\n",
    "\n",
    "    Args:\n",
    "    row (Series): A row from the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    Series: Modified row with updated ISO3 and Country/Territory.\n",
    "    \"\"\"\n",
    "    iso = row[\"ISO3\"]\n",
    "    country = row[\"Country / Territory\"]\n",
    "\n",
    "    # Special cases\n",
    "    if iso in [\"HKG\", \"MAC\"]:\n",
    "        return pd.Series({\"ISO3\": \"CHN\", \"Country / Territory\": \"China\"})\n",
    "    elif iso == \"ANT\":\n",
    "        return pd.Series({\"ISO3\": \"NLD\", \"Country / Territory\": \"Netherlands\"})\n",
    "    elif iso == \"SCG\":\n",
    "        # For Serbia and Montenegro, we need more information to decide\n",
    "        # For now, we'll keep it as is and flag it for manual review\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"ISO3\": \"SCG\",\n",
    "                \"Country / Territory\": \"Serbia and Montenegro (flagged for review)\",\n",
    "            }\n",
    "        )\n",
    "    elif iso == \"AB9\":\n",
    "        return pd.Series({\"ISO3\": \"SDN\", \"Country / Territory\": \"Sudan\"})\n",
    "    elif iso == \"XKX\":\n",
    "        return pd.Series({\"ISO3\": \"XKO\", \"Country / Territory\": \"Kosovo\"})\n",
    "\n",
    "    # If no special case, return the original values\n",
    "    return pd.Series({\"ISO3\": iso, \"Country / Territory\": country})\n",
    "\n",
    "\n",
    "# Apply the function to modify ISO3 and Country/Territory\n",
    "df_filtered[[\"ISO3\", \"Country / Territory\"]] = df_filtered.apply(\n",
    "    modify_iso_and_country, axis=1\n",
    ")\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "print(\"\\nFirst few rows after ISO3 and Country/Territory modification:\")\n",
    "display(df_filtered[[\"ISO3\", \"Country / Territory\"]].head())\n",
    "\n",
    "# Count the number of rows for each unique ISO3 code\n",
    "print(\"\\nNumber of rows for each unique ISO3 code:\")\n",
    "display(df_filtered[\"ISO3\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered[df_filtered[\"ISO3\"] == \"XKX\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the IDMC admin area mapping Excel file\n",
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file\n",
    "idmc_mapping_df = pd.read_excel(\"./IDMC_admin_area_mapping.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to retain only ISO3 and columns with prefix 'GADM_'\n",
    "gadm_columns = [\n",
    "    col\n",
    "    for col in idmc_mapping_df.columns\n",
    "    if col.startswith(\"GADM_\") or col == \"ISO3\"\n",
    "]\n",
    "filtered_idmc_mapping_df = idmc_mapping_df[gadm_columns]\n",
    "filtered_idmc_mapping_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_idmc_mapping_df[filtered_idmc_mapping_df[\"ISO3\"] == \"XKO\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine location names from GADM columns and group by admin levels\n",
    "def combine_locations(group):\n",
    "    \"\"\"\n",
    "    Combine location names from GADM columns and group by admin levels for each ISO3 group.\n",
    "\n",
    "    Args:\n",
    "    group (DataFrame): A group of rows with the same ISO3 code.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing combined and deduplicated location lists for each admin level.\n",
    "    \"\"\"\n",
    "    country_names = set()\n",
    "    admin1_names = set()\n",
    "    admin2_names = set()\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        for col in row.index:\n",
    "            if col.startswith(\"GADM_\") and isinstance(row[col], str):\n",
    "                names = set(\n",
    "                    name.strip()\n",
    "                    for name in row[col].split(\"|\")\n",
    "                    if name.strip()\n",
    "                )\n",
    "                if col in [\"GADM_Country\", \"GADM_Country_Alt\"]:\n",
    "                    country_names.update(names)\n",
    "                elif col in [\n",
    "                    \"GADM_Admin1\",\n",
    "                    \"GADM_Admin1_Alt\",\n",
    "                    \"GADM_Admin1_Local\",\n",
    "                ]:\n",
    "                    admin1_names.update(names)\n",
    "                elif col in [\n",
    "                    \"GADM_Admin2\",\n",
    "                    \"GADM_Admin2_Alt\",\n",
    "                    \"GADM_Admin2_Local\",\n",
    "                ]:\n",
    "                    admin2_names.update(names)\n",
    "\n",
    "    return pd.Series(\n",
    "        {\n",
    "            \"country_list\": list(country_names) if country_names else None,\n",
    "            \"admin1_list\": list(admin1_names) if admin1_names else None,\n",
    "            \"admin2_list\": list(admin2_names) if admin2_names else None,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# Group by ISO3 and apply the combine_locations function\n",
    "grouped_df = (\n",
    "    filtered_idmc_mapping_df.groupby(\"ISO3\")\n",
    "    .apply(combine_locations)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Rename columns\n",
    "grouped_df.columns = [\"ISO3\", \"country_list\", \"admin1_list\", \"admin2_list\"]\n",
    "\n",
    "# Remove rows where all location lists are None\n",
    "grouped_df = grouped_df.dropna(\n",
    "    subset=[\"country_list\", \"admin1_list\", \"admin2_list\"], how=\"all\"\n",
    ")\n",
    "\n",
    "# Display the first few rows of the transformed DataFrame\n",
    "print(\"\\nFirst few rows of the transformed DataFrame:\")\n",
    "display(grouped_df.head())\n",
    "\n",
    "# Print the shape of the resulting DataFrame\n",
    "print(f\"\\nShape of the resulting DataFrame: {grouped_df.shape}\")\n",
    "\n",
    "# Print the number of non-null values for each location list column\n",
    "print(\"\\nNumber of non-null values in each location list column:\")\n",
    "print(grouped_df[[\"country_list\", \"admin1_list\", \"admin2_list\"]].notna().sum())\n",
    "\n",
    "# Print an example of the combined lists for a specific country (e.g., Afghanistan)\n",
    "print(\"\\nExample of combined lists for Afghanistan:\")\n",
    "display(grouped_df[grouped_df[\"ISO3\"] == \"AFG\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df[grouped_df[\"ISO3\"] == \"XKO\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def find_location_mentions(input_string, iso3, location_df):\n",
    "    \"\"\"\n",
    "    Find location mentions in the input string for a specific ISO3 code.\n",
    "\n",
    "    Args:\n",
    "    input_string (str): The input string to search for location mentions.\n",
    "    iso3 (str): The ISO3 code to filter locations.\n",
    "    location_df (DataFrame): DataFrame containing location lists for each ISO3 code.\n",
    "\n",
    "    Returns:\n",
    "    list: List of tuples containing (location, admin_level) for matched locations.\n",
    "    \"\"\"\n",
    "    iso3 = iso3.strip()\n",
    "    # Handle special cases\n",
    "    if iso3 in [\"HKG\", \"MAC\"]:\n",
    "        iso3 = \"CHN\"\n",
    "    elif iso3 == \"ANT\":\n",
    "        iso3 = \"NLD\"\n",
    "    elif iso3 == \"SCG\":\n",
    "        # For Serbia and Montenegro, we'll check both countries\n",
    "        srb_locations = find_location_mentions(\n",
    "            input_string, \"SRB\", location_df\n",
    "        )\n",
    "        mne_locations = find_location_mentions(\n",
    "            input_string, \"MNE\", location_df\n",
    "        )\n",
    "        return srb_locations + mne_locations\n",
    "    elif iso3 == \"AB9\":\n",
    "        return [(\"Abyei Area\", \"admin2\")]\n",
    "    elif iso3 in \"XKX\":\n",
    "        print(iso3)\n",
    "        iso3 = \"XKO\"  # Assuming XKO is the ISO3 code for Kosovo in your location_df\n",
    "\n",
    "    # Get location lists for the specified ISO3 code\n",
    "    country_row = location_df[location_df[\"ISO3\"] == iso3]\n",
    "    if country_row.empty:\n",
    "        print(f\"No matching ISO3 code found for {iso3}\")\n",
    "        return []  # Return an empty list if no matching ISO3 is found\n",
    "\n",
    "    country_row = country_row.iloc[0]\n",
    "    country_list = country_row[\"country_list\"] or []\n",
    "    admin1_list = country_row[\"admin1_list\"] or []\n",
    "    admin2_list = country_row[\"admin2_list\"] or []\n",
    "\n",
    "    # For HKG and MAC, add them to admin1 list\n",
    "    if iso3 == \"CHN\" and (\n",
    "        input_string.lower().find(\"hong kong\") != -1\n",
    "        or input_string.lower().find(\"macau\") != -1\n",
    "    ):\n",
    "        admin1_list.extend([\"Hong Kong\", \"Macau\"])\n",
    "\n",
    "    # Combine all locations\n",
    "    all_locations = (\n",
    "        [(loc, \"country\") for loc in country_list]\n",
    "        + [(loc, \"admin1\") for loc in admin1_list]\n",
    "        + [(loc, \"admin2\") for loc in admin2_list]\n",
    "    )\n",
    "\n",
    "    # Sort locations by length (longest first) to ensure longer matches take precedence\n",
    "    sorted_locations = sorted(\n",
    "        all_locations, key=lambda x: len(x[0]), reverse=True\n",
    "    )\n",
    "\n",
    "    # Create a regex pattern that matches any of the locations as whole words/phrases\n",
    "    pattern = (\n",
    "        r\"\\b(?:\"\n",
    "        + \"|\".join(re.escape(loc[0]) for loc in sorted_locations)\n",
    "        + r\")\\b\"\n",
    "    )\n",
    "\n",
    "    # Perform case-insensitive search\n",
    "    matches = re.findall(pattern, input_string, re.IGNORECASE)\n",
    "\n",
    "    # Return unique matches with their admin level\n",
    "    return list(\n",
    "        set(\n",
    "            (loc[0], loc[1])\n",
    "            for loc in sorted_locations\n",
    "            if loc[0].lower() in [m.lower() for m in matches]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# Function to extract location names from Event Name\n",
    "def extract_locations(event_name, iso3, location_df):\n",
    "    \"\"\"\n",
    "    Extract location names from the Event Name for a specific ISO3 code.\n",
    "\n",
    "    Args:\n",
    "    event_name (str): The Event Name to search for locations.\n",
    "    iso3 (str): The ISO3 code to filter locations.\n",
    "    location_df (DataFrame): DataFrame containing location lists for each ISO3 code.\n",
    "\n",
    "    Returns:\n",
    "    list: List of tuples containing (location, admin_level) for matched locations.\n",
    "    \"\"\"\n",
    "    return find_location_mentions(event_name, iso3, location_df)\n",
    "\n",
    "\n",
    "# Apply the function to create a new column with extracted locations\n",
    "df_filtered[\"Extracted_Locations\"] = df_filtered.apply(\n",
    "    lambda row: extract_locations(row[\"Event Name\"], row[\"ISO3\"], grouped_df),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "print(\"\\nFirst few rows after location extraction:\")\n",
    "display(df_filtered[[\"ISO3\", \"Event Name\", \"Extracted_Locations\"]].head(10))\n",
    "\n",
    "# Count the number of events with extracted locations\n",
    "events_with_locations = (\n",
    "    df_filtered[\"Extracted_Locations\"].apply(lambda x: len(x) > 0).sum()\n",
    ")\n",
    "print(f\"\\nNumber of events with extracted locations: {events_with_locations}\")\n",
    "print(\n",
    "    f\"Percentage of events with extracted locations: {events_with_locations / len(df_filtered) * 100:.2f}%\"\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "example_iso3 = \"USA\"\n",
    "example_event = \"Earthquake - North - Los Angeles 20100101\"\n",
    "example_locations = extract_locations(example_event, example_iso3, grouped_df)\n",
    "print(\n",
    "    f\"\\nExample extraction for ISO3 '{example_iso3}' and event '{example_event}':\"\n",
    ")\n",
    "print(example_locations)\n",
    "\n",
    "# Print ISO3 codes that are in df_filtered but not in grouped_df\n",
    "missing_iso3 = set(df_filtered[\"ISO3\"]) - set(grouped_df[\"ISO3\"])\n",
    "print(\"\\nISO3 codes in df_filtered but not in grouped_df:\")\n",
    "print(missing_iso3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered[df_filtered[\"ISO3\"] == \"XKX\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "source": [
    "- Transform ISO3, Admin name, and Admin level to GADM's standard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to transform location names and expand admin levels\n",
    "# def transform_locations(extracted_locations, iso3, filtered_idmc_mapping_df):\n",
    "#     \"\"\"\n",
    "#     Transform location names to GADM's main names and expand admin levels.\n",
    "\n",
    "#     Args:\n",
    "#     extracted_locations (list): List of tuples containing (location, admin_level).\n",
    "#     iso3 (str): The ISO3 code of the country.\n",
    "#     filtered_idmc_mapping_df (DataFrame): DataFrame containing GADM location mappings.\n",
    "\n",
    "#     Returns:\n",
    "#     dict: A dictionary with transformed locations for each admin level.\n",
    "#     \"\"\"\n",
    "#     country_row = filtered_idmc_mapping_df[filtered_idmc_mapping_df['ISO3'] == iso3]\n",
    "#     if country_row.empty:\n",
    "#         return {'admin_level_0': [], 'admin_level_1': [], 'admin_level_2': []}\n",
    "\n",
    "#     result = {'admin_level_0': [], 'admin_level_1': [], 'admin_level_2': []}\n",
    "\n",
    "#     for location, admin_level in extracted_locations:\n",
    "#         if admin_level == 'country':\n",
    "#             country_name = country_row['GADM_Country'].iloc[0]\n",
    "#             if country_name not in result['admin_level_0']:\n",
    "#                 result['admin_level_0'].append(country_name)\n",
    "#         elif admin_level == 'admin1':\n",
    "#             admin1_match = country_row[country_row['GADM_Admin1'].str.contains(location, case=False, na=False) |\n",
    "#                                        country_row['GADM_Admin1_Alt'].str.contains(location, case=False, na=False) |\n",
    "#                                        country_row['GADM_Admin1_Local'].str.contains(location, case=False, na=False)]\n",
    "#             if not admin1_match.empty:\n",
    "#                 admin1_name = admin1_match['GADM_Admin1'].iloc[0]\n",
    "#                 if admin1_name not in result['admin_level_1']:\n",
    "#                     result['admin_level_1'].append(admin1_name)\n",
    "#         elif admin_level == 'admin2':\n",
    "#             admin2_match = country_row[country_row['GADM_Admin2'].str.contains(location, case=False, na=False) |\n",
    "#                                        country_row['GADM_Admin2_Alt'].str.contains(location, case=False, na=False) |\n",
    "#                                        country_row['GADM_Admin2_Local'].str.contains(location, case=False, na=False)]\n",
    "#             if not admin2_match.empty:\n",
    "#                 admin2_name = admin2_match['GADM_Admin2'].iloc[0]\n",
    "#                 if admin2_name not in result['admin_level_2']:\n",
    "#                     result['admin_level_2'].append(admin2_name)\n",
    "\n",
    "#     return result\n",
    "\n",
    "# # Apply the transformation to the DataFrame\n",
    "# df_filtered[['admin_level_0', 'admin_level_1', 'admin_level_2']] = df_filtered.apply(\n",
    "#     lambda row: pd.Series(transform_locations(row['Extracted_Locations'], row['ISO3'], filtered_idmc_mapping_df)),\n",
    "#     axis=1\n",
    "# )\n",
    "\n",
    "# # Display the first few rows to verify the changes\n",
    "# print(\"\\nFirst few rows after location transformation:\")\n",
    "# display(df_filtered[['ISO3', 'Event Name', 'Extracted_Locations', 'admin_level_0', 'admin_level_1', 'admin_level_2']].head(10))\n",
    "\n",
    "# # Count the number of events with transformed locations\n",
    "# events_with_transformed_locations = df_filtered[['admin_level_0', 'admin_level_1', 'admin_level_2']].apply(lambda x: x.apply(len) > 0).any(axis=1).sum()\n",
    "# print(f\"\\nNumber of events with transformed locations: {events_with_transformed_locations}\")\n",
    "# print(f\"Percentage of events with transformed locations: {events_with_transformed_locations / len(df_filtered) * 100:.2f}%\")\n",
    "\n",
    "# # Print the number of non-empty lists for each admin level column\n",
    "# print(\"\\nNumber of non-empty lists in each admin level column:\")\n",
    "# print(df_filtered[['admin_level_0', 'admin_level_1', 'admin_level_2']].apply(lambda x: x.apply(len) > 0).sum())\n",
    "\n",
    "# # Check for rows with more than one admin_level_0 name\n",
    "# rows_with_multiple_admin0 = df_filtered[df_filtered['admin_level_0'].apply(len) > 1]\n",
    "# print(f\"\\nNumber of rows with more than one admin_level_0 name: {len(rows_with_multiple_admin0)}\")\n",
    "\n",
    "# if len(rows_with_multiple_admin0) > 0:\n",
    "#     print(\"\\nSample rows with multiple admin_level_0 names:\")\n",
    "#     display(rows_with_multiple_admin0[['ISO3', 'Event Name', 'Extracted_Locations', 'admin_level_0']].head())\n",
    "\n",
    "# Function to transform location names and expand admin levels\n",
    "def transform_locations(\n",
    "    original_country, extracted_locations, iso3, filtered_idmc_mapping_df\n",
    "):\n",
    "    \"\"\"\n",
    "    Transform location names to GADM's main names and expand admin levels.\n",
    "\n",
    "    Args:\n",
    "    extracted_locations (list): List of tuples containing (location, admin_level).\n",
    "    iso3 (str): The ISO3 code of the country.\n",
    "    filtered_idmc_mapping_df (DataFrame): DataFrame containing GADM location mappings.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with transformed locations for each admin level.\n",
    "    \"\"\"\n",
    "    country_row = filtered_idmc_mapping_df[\n",
    "        filtered_idmc_mapping_df[\"ISO3\"] == iso3\n",
    "    ]\n",
    "    if country_row.empty:\n",
    "        return {\n",
    "            \"Country / Territory\": None,\n",
    "            \"admin_level_1\": [],\n",
    "            \"admin_level_2\": [],\n",
    "        }\n",
    "\n",
    "    country = original_country\n",
    "    if original_country != country_row[\"GADM_Country\"].iloc[0]:\n",
    "        print(\n",
    "            f\"original: {original_country}, GADM: {country_row['GADM_Country'].iloc[0]}\"\n",
    "        )\n",
    "        country = country_row[\"GADM_Country\"].iloc[0]\n",
    "\n",
    "    result = {\n",
    "        \"Country / Territory\": country,\n",
    "        \"admin_level_1\": [],\n",
    "        \"admin_level_2\": [],\n",
    "    }\n",
    "\n",
    "    for location, admin_level in extracted_locations:\n",
    "        if admin_level == \"admin1\":\n",
    "            admin1_match = country_row[\n",
    "                country_row[\"GADM_Admin1\"].str.contains(\n",
    "                    location, case=False, na=False\n",
    "                )\n",
    "                | country_row[\"GADM_Admin1_Alt\"].str.contains(\n",
    "                    location, case=False, na=False\n",
    "                )\n",
    "                | country_row[\"GADM_Admin1_Local\"].str.contains(\n",
    "                    location, case=False, na=False\n",
    "                )\n",
    "            ]\n",
    "            if not admin1_match.empty:\n",
    "                admin1_name = admin1_match[\"GADM_Admin1\"].iloc[0]\n",
    "                if admin1_name not in result[\"admin_level_1\"]:\n",
    "                    result[\"admin_level_1\"].append(admin1_name)\n",
    "        elif admin_level == \"admin2\":\n",
    "            admin2_match = country_row[\n",
    "                country_row[\"GADM_Admin2\"].str.contains(\n",
    "                    location, case=False, na=False\n",
    "                )\n",
    "                | country_row[\"GADM_Admin2_Alt\"].str.contains(\n",
    "                    location, case=False, na=False\n",
    "                )\n",
    "                | country_row[\"GADM_Admin2_Local\"].str.contains(\n",
    "                    location, case=False, na=False\n",
    "                )\n",
    "            ]\n",
    "            if not admin2_match.empty:\n",
    "                admin2_name = admin2_match[\"GADM_Admin2\"].iloc[0]\n",
    "                if admin2_name not in result[\"admin_level_2\"]:\n",
    "                    result[\"admin_level_2\"].append(admin2_name)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Apply the transformation to the DataFrame\n",
    "df_filtered[[\"Country / Territory\", \"admin_level_1\", \"admin_level_2\"]] = (\n",
    "    df_filtered.apply(\n",
    "        lambda row: pd.Series(\n",
    "            transform_locations(\n",
    "                row[\"Country / Territory\"],\n",
    "                row[\"Extracted_Locations\"],\n",
    "                row[\"ISO3\"],\n",
    "                filtered_idmc_mapping_df,\n",
    "            )\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "print(\"\\nFirst few rows after location transformation:\")\n",
    "display(\n",
    "    df_filtered[\n",
    "        [\n",
    "            \"ISO3\",\n",
    "            \"Event Name\",\n",
    "            \"Extracted_Locations\",\n",
    "            \"admin_level_1\",\n",
    "            \"admin_level_2\",\n",
    "        ]\n",
    "    ].head(10)\n",
    ")\n",
    "\n",
    "# Count the number of events with transformed locations\n",
    "events_with_transformed_locations = (\n",
    "    df_filtered[[\"admin_level_1\", \"admin_level_2\"]]\n",
    "    .apply(lambda x: x.apply(len) > 0)\n",
    "    .any(axis=1)\n",
    "    .sum()\n",
    ")\n",
    "print(\n",
    "    f\"\\nNumber of events with transformed locations: {events_with_transformed_locations}\"\n",
    ")\n",
    "print(\n",
    "    f\"Percentage of events with transformed locations: {events_with_transformed_locations / len(df_filtered) * 100:.2f}%\"\n",
    ")\n",
    "\n",
    "# Print the number of non-empty lists for each admin level column\n",
    "print(\"\\nNumber of non-empty lists in each admin level column:\")\n",
    "print(\n",
    "    df_filtered[[\"admin_level_1\", \"admin_level_2\"]]\n",
    "    .apply(lambda x: x.apply(len) > 0)\n",
    "    .sum()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered[df_filtered[\"Country / Territory\"].isna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_col = df_filtered[df_filtered[\"ISO3\"] == \"TWN\"]\n",
    "df_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for rows where Extracted_Locations is an empty list and display the first few rows\n",
    "df_filtered[df_filtered[\"Extracted_Locations\"].apply(lambda x: len(x) == 0)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Map Hazard classification to EMDAT's classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the IDMC admin area mapping Excel file\n",
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file\n",
    "disaster_hazard_type_mapping_df = pd.read_excel(\n",
    "    \"./Disaster Hazard Type Map.xlsx\"\n",
    ")\n",
    "# Filter rows where either \"Hazard Type\" or \"Hazard Sub Type\" is not null\n",
    "disaster_hazard_type_mapping_df = disaster_hazard_type_mapping_df[\n",
    "    disaster_hazard_type_mapping_df[\"Hazard Type\"].notna()\n",
    "    | disaster_hazard_type_mapping_df[\"Hazard Sub Type\"].notna()\n",
    "]\n",
    "disaster_hazard_type_mapping_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for mapping Hazard Type and Sub Type to Disaster Type and Subtype\n",
    "hazard_to_disaster_map = {}\n",
    "for _, row in disaster_hazard_type_mapping_df.iterrows():\n",
    "    hazard_key = (row[\"Hazard Type\"], row[\"Hazard Sub Type\"])\n",
    "    disaster_value = (row[\"Disaster Type\"], row[\"Disaster Subtype\"])\n",
    "    hazard_to_disaster_map[hazard_key] = disaster_value\n",
    "\n",
    "\n",
    "# Function to map hazard types to disaster types\n",
    "def map_hazard_to_disaster(hazard_type, hazard_sub_type):\n",
    "    \"\"\"\n",
    "    Maps hazard type and sub-type to disaster type and subtype.\n",
    "\n",
    "    Args:\n",
    "    hazard_type (str): The hazard type.\n",
    "    hazard_sub_type (str): The hazard sub-type.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the mapped disaster type and subtype.\n",
    "    \"\"\"\n",
    "    return hazard_to_disaster_map.get(\n",
    "        (hazard_type, hazard_sub_type), (hazard_type, hazard_sub_type)\n",
    "    )\n",
    "\n",
    "\n",
    "# Apply the mapping to df_filtered\n",
    "df_filtered[[\"Disaster Type\", \"Disaster Subtype\"]] = df_filtered.apply(\n",
    "    lambda row: pd.Series(\n",
    "        map_hazard_to_disaster(row[\"Hazard Type\"], row[\"Hazard Sub Type\"])\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Display the first few rows of the updated df_filtered to verify the changes\n",
    "print(\n",
    "    df_filtered[\n",
    "        [\"Hazard Type\", \"Hazard Sub Type\", \"Disaster Type\", \"Disaster Subtype\"]\n",
    "    ].head()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rename columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns based on the provided mapping\n",
    "column_mapping = {\n",
    "    \"Disaster Type\": \"disaster_type\",\n",
    "    \"Disaster Subtype\": \"disaster_subtype\",\n",
    "    \"ISO3\": \"iso3_code\",\n",
    "    \"Country / Territory\": \"admin_level_0\",\n",
    "    \"Disaster Internal Displacements\": \"disaster_internal_displacements\",\n",
    "    \"Date of Event (start)\": \"start_date\",\n",
    "}\n",
    "\n",
    "# Rename the columns in df_filtered\n",
    "df_filtered.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# Display the first few rows to verify changes\n",
    "print(\"First few rows after renaming columns:\")\n",
    "display(df_filtered[list(column_mapping.values())].head())\n",
    "\n",
    "# Display column names to confirm changes\n",
    "print(\"\\nUpdated column names:\")\n",
    "print(df_filtered.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Merge admin_level_0_original column with admin_level_0\n",
    "# df_filtered['admin_level_0'] = df_filtered.apply(\n",
    "#     lambda row: row['admin_level_0'] if pd.notnull(row['admin_level_0']) else row['admin_level_0_original'],\n",
    "#     axis=1\n",
    "# )\n",
    "\n",
    "# # Drop the original column as it's no longer needed\n",
    "# df_filtered.drop('admin_level_0_original', axis=1, inplace=True)\n",
    "\n",
    "# # Display the first few rows to verify changes\n",
    "# print(\"First few rows after merging admin_level_0 columns:\")\n",
    "# display(df_filtered[['iso3_code', 'admin_level_0']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the columns to be included in the metadata\n",
    "metadata_columns = [\n",
    "    \"Event Name\",\n",
    "    \"Event Codes (Code:Type)\",\n",
    "    \"Year\",\n",
    "    \"Hazard Type\",\n",
    "    \"Hazard Sub Type\",\n",
    "]\n",
    "\n",
    "\n",
    "# Function to create metadata JSON\n",
    "def create_metadata(row):\n",
    "    \"\"\"\n",
    "    Create a JSON string containing metadata from specified columns.\n",
    "\n",
    "    Args:\n",
    "    row (pandas.Series): A row from the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    str: JSON string containing metadata.\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        col: row[col] for col in metadata_columns if pd.notna(row[col])\n",
    "    }\n",
    "    return json.dumps(metadata)\n",
    "\n",
    "\n",
    "# Create the metadata column\n",
    "df_filtered[\"metadata\"] = df_filtered.apply(create_metadata, axis=1)\n",
    "\n",
    "# Remove the original columns that are now in metadata\n",
    "df_filtered = df_filtered.drop(columns=metadata_columns)\n",
    "\n",
    "print(\"\\nUpdated column names after removing metadata columns:\")\n",
    "print(df_filtered.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'source' column with value 'IDMC'\n",
    "df_filtered[\"source\"] = \"IDMC\"\n",
    "\n",
    "# Display the first few rows to verify the new column\n",
    "print(\"First few rows after adding 'source' column:\")\n",
    "display(df_filtered[[\"source\"] + df_filtered.columns[:-1].tolist()].head())\n",
    "\n",
    "# Confirm the new column is added\n",
    "print(\"\\nUpdated column names:\")\n",
    "print(df_filtered.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'Glide Number' column to 'GLIDE'\n",
    "df_filtered = df_filtered.rename(columns={\"Glide Number\": \"GLIDE\"})\n",
    "# Rename columns with underscores instead of spaces\n",
    "df_filtered = df_filtered.rename(\n",
    "    columns={\n",
    "        \"Local Identifier\": \"Local_Identifier\",\n",
    "        \"IFRC Appeal ID\": \"IFRC_Appeal_ID\",\n",
    "        \"Government Assigned Identifier\": \"Government_Assigned_Identifier\",\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Drop 'Extracted_Locations' column\n",
    "df_filtered = df_filtered.drop(columns=[\"Extracted_Locations\"])\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "print(\n",
    "    \"First few rows after renaming 'Glide Number' to 'GLIDE' and dropping 'Extracted_Locations':\"\n",
    ")\n",
    "display(df_filtered.head())\n",
    "\n",
    "# Confirm the updated column names\n",
    "print(\"\\nUpdated column names:\")\n",
    "print(df_filtered.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "\n",
    "def generate_short_uuid():\n",
    "    \"\"\"Generate a short UUID.\"\"\"\n",
    "    return str(uuid.uuid4())[:8]\n",
    "\n",
    "\n",
    "def create_event_name(row):\n",
    "    \"\"\"\n",
    "    Create an event name based on the specified pattern.\n",
    "\n",
    "    Args:\n",
    "    row (pandas.Series): A row from the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    str: Event name string.\n",
    "    \"\"\"\n",
    "    # Define default values for missing data\n",
    "    iso3 = (\n",
    "        row[\"iso3_code\"] if pd.notna(row[\"iso3_code\"]) else \"UNKNOWN-ISO3-CODE\"\n",
    "    )\n",
    "    disaster_type = (\n",
    "        row[\"disaster_type\"]\n",
    "        if pd.notna(row[\"disaster_type\"])\n",
    "        else \"UNKNOWN-DISASTER-TYPE\"\n",
    "    )\n",
    "    disaster_subtype = (\n",
    "        row[\"disaster_subtype\"]\n",
    "        if pd.notna(row[\"disaster_subtype\"])\n",
    "        else \"UNKNOWN-DISASTER-SUBTYPE\"\n",
    "    )\n",
    "\n",
    "    # Convert start_date to string format YYYYMMDD\n",
    "    start_time = (\n",
    "        row[\"start_date\"].strftime(\"%Y%m%d\")\n",
    "        if pd.notna(row[\"start_date\"])\n",
    "        else \"UNKNOWN-START-DATE\"\n",
    "    )\n",
    "\n",
    "    # Generate short UUID\n",
    "    short_uuid = generate_short_uuid()\n",
    "\n",
    "    # Create event name\n",
    "    event_name = (\n",
    "        f\"{iso3}_{disaster_type}_{disaster_subtype}_{start_time}_{short_uuid}\"\n",
    "    )\n",
    "\n",
    "    return event_name\n",
    "\n",
    "\n",
    "# Create the event_name column\n",
    "df_filtered[\"event_name\"] = df_filtered.apply(create_event_name, axis=1)\n",
    "\n",
    "# Display the first few rows to verify the new event_name column\n",
    "print(\"First few rows after adding event_name column:\")\n",
    "display(df_filtered[[\"event_name\"] + df_filtered.columns[:-1].tolist()].head())\n",
    "\n",
    "# Confirm the new column is added\n",
    "print(\"\\nUpdated column names:\")\n",
    "print(df_filtered.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final df_filtered to a CSV file\n",
    "csv_filename = \"preprocessed_idmc.csv\"\n",
    "df_filtered.to_csv(csv_filename, index=False)\n",
    "print(f\"DataFrame saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "\n",
    "def insert_idmc_data(df):\n",
    "    \"\"\"\n",
    "    Insert the preprocessed IDMC data into the events_idmc table.\n",
    "\n",
    "    Args:\n",
    "    df (pandas.DataFrame): The preprocessed IDMC data.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Connect to the local PostgreSQL database\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=\"merge\",\n",
    "            user=\"postgres\",\n",
    "            password=getpass(\"Enter the database password: \"),\n",
    "            host=input(\"Enter the database host: \"),\n",
    "        )\n",
    "\n",
    "        # Create a cursor object\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Prepare the data for insertion\n",
    "        data = df.to_dict(\"records\")\n",
    "\n",
    "        # SQL query for inserting data\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO events_idmc (\n",
    "            event_name, disaster_type, disaster_subtype, iso3_code, admin_level_0,\n",
    "            admin_level_1, admin_level_2, start_date, disaster_internal_displacements,\n",
    "            source, metadata, GLIDE, local_Identifier, IFRC_Appeal_ID, Government_Assigned_Identifier\n",
    "        ) VALUES %s\n",
    "        ON CONFLICT (event_name, iso3_code, start_date) DO NOTHING\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert data to tuple format for psycopg2, handling potential integer overflow\n",
    "        values = [\n",
    "            (\n",
    "                row[\"event_name\"],\n",
    "                row[\"disaster_type\"],\n",
    "                row[\"disaster_subtype\"],\n",
    "                row[\"iso3_code\"],\n",
    "                row[\"admin_level_0\"] if row[\"admin_level_0\"] else None,\n",
    "                row[\"admin_level_1\"],\n",
    "                row[\"admin_level_2\"],\n",
    "                row[\"start_date\"],\n",
    "                int(row[\"disaster_internal_displacements\"])\n",
    "                if pd.notna(row[\"disaster_internal_displacements\"])\n",
    "                else None,\n",
    "                row[\"source\"],\n",
    "                row[\"metadata\"],\n",
    "                row[\"GLIDE\"] if isinstance(row[\"GLIDE\"], list) else None,\n",
    "                row[\"Local_Identifier\"]\n",
    "                if isinstance(row[\"Local_Identifier\"], list)\n",
    "                else None,\n",
    "                row[\"IFRC_Appeal_ID\"]\n",
    "                if isinstance(row[\"IFRC_Appeal_ID\"], list)\n",
    "                else None,\n",
    "                row[\"Government_Assigned_Identifier\"]\n",
    "                if isinstance(row[\"Government_Assigned_Identifier\"], list)\n",
    "                else None,\n",
    "            )\n",
    "            for row in data\n",
    "        ]\n",
    "\n",
    "        # Execute the insert query\n",
    "        execute_values(cur, insert_query, values)\n",
    "\n",
    "        # Commit the changes\n",
    "        conn.commit()\n",
    "\n",
    "        print(\"Data inserted successfully into events_idmc table.\")\n",
    "\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(f\"Error inserting data into events_idmc table: {error}\")\n",
    "\n",
    "    finally:\n",
    "        # Close the cursor and connection\n",
    "        if cur:\n",
    "            cur.close()\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "\n",
    "# Call the function to insert the data\n",
    "insert_idmc_data(df_filtered)\n",
    "\n",
    "# Print a message to confirm the operation is complete\n",
    "print(\"IDMC data insertion process completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about the filtered DataFrame\n",
    "print(\"Filtered DataFrame Info:\")\n",
    "df_filtered.info()\n",
    "\n",
    "# Display the first few rows of the filtered DataFrame\n",
    "print(\"\\nFirst few rows of the filtered DataFrame:\")\n",
    "display(df_filtered.head())\n",
    "\n",
    "# Display summary statistics for the filtered DataFrame\n",
    "print(\"\\nSummary statistics for the filtered DataFrame:\")\n",
    "display(df_filtered.describe())\n",
    "\n",
    "# Count non-null values for each column in the filtered DataFrame\n",
    "print(\"\\nNon-null value counts for each column:\")\n",
    "display(df_filtered.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
